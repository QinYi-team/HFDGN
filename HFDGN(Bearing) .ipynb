{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import  Counter\n",
    "import math\n",
    "import logging\n",
    "logging.disable(30)\n",
    "from test_step   import test\n",
    "from train_step4 import train4\n",
    "from MINE import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one client\n",
    "def weight_add1(model1,test_model):\n",
    "    for i in range(len(model1.weights)):\n",
    "        tf.compat.v1.assign(test_model.weights[i],model1.weights[i])\n",
    "    return model1,test_model\n",
    "# two clients\n",
    "def weight_add2(model1,model2,test_model,w1,w2):\n",
    "    for i in range(len(model1.weights)):\n",
    "        new_weights=tf.reduce_sum([model1.weights[i]*w1,model2.weights[i]*w2],axis=0)\n",
    "        tf.compat.v1.assign(model1.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model2.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(test_model.weights[i],new_weights)\n",
    "    return model1,model2,test_model\n",
    "# three clients\n",
    "def weight_add3(model1,model2,model3,test_model,w1,w2,w3):\n",
    "    for i in range(len(model1.weights)):\n",
    "        new_weights=tf.reduce_sum([model1.weights[i]*w1,model2.weights[i]*w2,model3.weights[i]*w3],axis=0)\n",
    "        tf.compat.v1.assign(model1.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model2.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model3.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(test_model.weights[i],new_weights)\n",
    "    return model1,model2,model3,test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_name(var):\n",
    "    import inspect\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    var_name_list = [var_name for var_name, var_val in callers_local_vars if var_val is var] \n",
    "    if len(var_name_list) > 0:\n",
    "        return var_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_preprocess (data):\n",
    "    data=(data-data.min(axis=1).reshape((len(data),1)))/(data.max(axis=1).reshape((len(data),1))-data.min(axis=1).reshape((len(data),1)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 5), (500, 5), (500, 5), (500, 5)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n",
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 4), (500, 4), (500, 4), (500, 4)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n",
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 4), (500, 4), (500, 4), (500, 4)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n",
      "0.5341327554135927 0.5920691489895694 0.5684197615146983 0.7179127363638874\n",
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 4), (500, 4), (500, 4), (500, 4)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "#Gear_dataset\n",
    "G_data1=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load0.csv\").values\n",
    "G_label1=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load0.csv\").values\n",
    "G_data2=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load1.csv\").values\n",
    "G_label2=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load1.csv\").values\n",
    "G_data3=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load2.csv\").values\n",
    "G_label3=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load2.csv\").values\n",
    "G_data4=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load3.csv\").values\n",
    "G_label4=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load3.csv\").values\n",
    "G_data1=normalized_preprocess(G_data1) \n",
    "G_data2=normalized_preprocess(G_data2)\n",
    "G_data3=normalized_preprocess(G_data3)\n",
    "G_data4=normalized_preprocess(G_data4)\n",
    "G_data1=tf.expand_dims(G_data1,axis=-1)\n",
    "G_data2=tf.expand_dims(G_data2,axis=-1)\n",
    "G_data3=tf.expand_dims(G_data3,axis=-1)\n",
    "G_data4=tf.expand_dims(G_data4,axis=-1)\n",
    "G_train_dataset=tf.data.Dataset.from_tensor_slices((G_data2,G_data1,G_data3,G_data4,G_label2,G_label1,G_label3,G_label4))\n",
    "G_train_dataset=G_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(G_train_dataset)\n",
    "\n",
    "\n",
    "#Bearing_dataset\n",
    "B_data1=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load0_0.007_test.csv').values\n",
    "B_label1=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load0_0.007_test.csv').values\n",
    "B_data2=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load1_0.007_test.csv').values\n",
    "B_label2=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load1_0.007_test.csv').values\n",
    "B_data3=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load2_0.007_test.csv').values\n",
    "B_label3=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load2_0.007_test.csv').values\n",
    "B_data4=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load3_0.007_test.csv').values\n",
    "B_label4=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load3_0.007_test.csv').values\n",
    "B_data1=normalized_preprocess(B_data1) \n",
    "B_data2=normalized_preprocess(B_data2)\n",
    "B_data3=normalized_preprocess(B_data3)\n",
    "B_data4=normalized_preprocess(B_data4)\n",
    "B_data1=tf.expand_dims(B_data1,axis=-1)\n",
    "B_data2=tf.expand_dims(B_data2,axis=-1)\n",
    "B_data3=tf.expand_dims(B_data3,axis=-1)\n",
    "B_data4=tf.expand_dims(B_data4,axis=-1)\n",
    "B_train_dataset=tf.data.Dataset.from_tensor_slices((B_data1,B_data2,B_data3,B_data4,B_label1,B_label2,B_label3,B_label4))\n",
    "B1_train_dataset=B_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(B1_train_dataset)\n",
    "\n",
    "\n",
    "#Bearing_dataset\n",
    "B_data1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load0_2000_channel1_test.csv\").values\n",
    "B_label1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load0_2000_channel1_test.csv\").values\n",
    "B_data2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load1_2000_channel1_test.csv\").values\n",
    "B_label2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load1_2000_channel1_test.csv\").values\n",
    "B_data3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load2_2000_channel1_test.csv\").values\n",
    "B_label3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load2_2000_channel1_test.csv\").values\n",
    "B_data4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load3_1000_channel1_test.csv\").values\n",
    "B_label4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load3_1000_channel1_test.csv\").values\n",
    "B_data1=normalized_preprocess(B_data1) \n",
    "B_data2=normalized_preprocess(B_data2)\n",
    "B_data3=normalized_preprocess(B_data3)\n",
    "B_data4=normalized_preprocess(B_data4)\n",
    "B_data1=tf.expand_dims(B_data1,axis=-1)\n",
    "B_data2=tf.expand_dims(B_data2,axis=-1)\n",
    "B_data3=tf.expand_dims(B_data3,axis=-1)\n",
    "B_data4=tf.expand_dims(B_data4,axis=-1)\n",
    "B_train_dataset=tf.data.Dataset.from_tensor_slices((B_data1,B_data2,B_data3,B_data4,B_label1,B_label2,B_label3,B_label4))\n",
    "B2_train_dataset=B_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(B2_train_dataset)\n",
    "\n",
    "\n",
    "#Bearing_dataset\n",
    "B_data1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load0_1000_0.3_test.csv\").values\n",
    "B_label1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load0_1000_0.3_test.csv\").values\n",
    "B_data2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load1_1000_0.3_test.csv\").values\n",
    "B_label2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load1_1000_0.3_test.csv\").values\n",
    "B_data3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load2_1000_0.3_test.csv\").values\n",
    "B_label3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load2_1000_0.3_test.csv\").values\n",
    "B_data4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load3_1000_0.3_test.csv\").values\n",
    "B_label4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load3_1000_0.3_test.csv\").values\n",
    "B_data1=normalized_preprocess(B_data1) \n",
    "B_data2=normalized_preprocess(B_data2)\n",
    "B_data3=normalized_preprocess(B_data3)\n",
    "B_data4=normalized_preprocess(B_data4)\n",
    "print(B_data1[0][0],B_data2[0][0],B_data3[0][0],B_data4[0][0])\n",
    "B_data1=tf.expand_dims(B_data1,axis=-1)\n",
    "B_data2=tf.expand_dims(B_data2,axis=-1)\n",
    "B_data3=tf.expand_dims(B_data3,axis=-1)\n",
    "B_data4=tf.expand_dims(B_data4,axis=-1)\n",
    "B_train_dataset=tf.data.Dataset.from_tensor_slices((B_data3,B_data1,B_data2,B_data4,B_label3,B_label1,B_label2,B_label4))\n",
    "B3_train_dataset=B_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(B3_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_extractor():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,kernel_size=16,strides=8,activation='relu',padding='same',input_shape=(3072,1)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv1D(filters=64,kernel_size=3,strides=1,activation='relu',padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same'),])\n",
    "    return model\n",
    "\n",
    "def Disentangler():\n",
    "    input_data=tf.keras.Input(shape=(48,128))\n",
    "    x1=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(input_data)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x1)\n",
    "    x1=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x1)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x1)    \n",
    "    x1=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x1)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    ci_output=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x1)\n",
    "    \n",
    "    x2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(input_data)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x2)\n",
    "    x2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x2)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x2)    \n",
    "    x2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x2)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    cr_output=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x2)\n",
    "    \n",
    "    model=tf.keras.Model(inputs=input_data, outputs=[ci_output,cr_output])\n",
    "    return model\n",
    "\n",
    "def Reconstructor():\n",
    "    ci_input=tf.keras.Input(shape=(6,128))\n",
    "    x1=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(ci_input)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x1)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x1)\n",
    "    ci_recon=tf.keras.layers.BatchNormalization()(x1)\n",
    "    \n",
    "    cr_input=tf.keras.Input(shape=(6,128))\n",
    "    x2=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(cr_input)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x2)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x2)\n",
    "    cr_recon=tf.keras.layers.BatchNormalization()(x2)\n",
    "    \n",
    "    recon_data=tf.keras.layers.Add()([ci_recon,cr_recon])\n",
    "    model=tf.keras.Model(inputs=[ci_input,cr_input], outputs=recon_data)\n",
    "    return model\n",
    "\n",
    "\n",
    "def Classifier(unit_number):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128,activation='relu',input_shape=(128,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(unit_number,activation='relu'),])\n",
    "    return model\n",
    "\n",
    "\n",
    "def NET():\n",
    "    model=tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(8,input_shape=(256,),activation='relu'),\n",
    "          tf.keras.layers.Dense(4,activation='relu'),\n",
    "          tf.keras.layers.Dense(1),])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comunication(comunication_number):\n",
    "    GAP=tf.keras.layers.GlobalAveragePooling1D()\n",
    "    G_feature_extractor=Feature_extractor()\n",
    "    G_disentangler=Disentangler()\n",
    "    G_reconstructor=Reconstructor()\n",
    "    G_classifier=Classifier(5)\n",
    "    G_net=NET()\n",
    "\n",
    "    B1_feature_extractor=Feature_extractor()\n",
    "    B1_disentangler=Disentangler()\n",
    "    B1_reconstructor=Reconstructor()\n",
    "    B1_classifier=Classifier(4)\n",
    "    B1_net=NET()\n",
    "\n",
    "    B2_feature_extractor=Feature_extractor()\n",
    "    B2_disentangler=Disentangler()\n",
    "    B2_reconstructor=Reconstructor()\n",
    "    B2_classifier=Classifier(4)\n",
    "    B2_net=NET() \n",
    "    \n",
    "    B3_feature_extractor=Feature_extractor()\n",
    "    B3_disentangler=Disentangler()\n",
    "    B3_reconstructor=Reconstructor()\n",
    "    B3_classifier=Classifier(4)\n",
    "    B3_net=NET()\n",
    "    \n",
    "    weights={\"w1\":[],\"w2\":[],\"w3\":[],\"test2\":[],\"test3\":[],\"test4\":[]}\n",
    "    for i in range(comunication_number):\n",
    "        print(\"*\"*100)\n",
    "        loss1=train4(B1_feature_extractor,B1_disentangler,B1_reconstructor,B1_classifier,B1_net,5,1,1,1,B1_train_dataset,100)\n",
    "        loss2=train4(B2_feature_extractor,B2_disentangler,B2_reconstructor,B2_classifier,B2_net,5,1,1,1,B2_train_dataset,100)\n",
    "        loss3=train4(G_feature_extractor,G_disentangler,G_reconstructor,G_classifier,G_net,5,1,1,1,G_train_dataset,100)\n",
    "        test_loss,orig_feature,label_feature=test(B3_feature_extractor,B3_disentangler,B3_reconstructor,B3_classifier,B3_net,5,1,1,1,B3_train_dataset,100)        \n",
    "        \n",
    "        ci1,cr1= B1_disentangler(orig_feature,training = True)\n",
    "        label_feature1=B1_classifier(GAP(cr1),training = True)\n",
    "        info1 = Estimator(label_feature1.shape[-1],label_feature.shape[-1]).backward(label_feature1,label_feature,2000)\n",
    "        \n",
    "        ci1,cr1= B2_disentangler(orig_feature,training = True)\n",
    "        label_feature2=B2_classifier(GAP(cr1),training = True)\n",
    "        info2 = Estimator(label_feature2.shape[-1],label_feature.shape[-1]).backward(label_feature2,label_feature,2000)\n",
    " \n",
    "        ci1,cr1= G_disentangler(orig_feature,training = True)\n",
    "        label_feature3=G_classifier(GAP(cr1),training = True)\n",
    "        info3 = Estimator(label_feature3.shape[-1],label_feature.shape[-1]).backward(label_feature3,label_feature,2000)\n",
    "        \n",
    "        c=0.2\n",
    "        w1=tf.nn.softmax([info1[-1]/c,info2[-1]/c,info3[-1]/c],axis=0)[0]\n",
    "        w2=tf.nn.softmax([info1[-1]/c,info2[-1]/c,info3[-1]/c],axis=0)[1]\n",
    "        w3=tf.nn.softmax([info1[-1]/c,info2[-1]/c,info3[-1]/c],axis=0)[2]\n",
    "        \n",
    "        B1_feature_extractor,B2_feature_extractor,G_feature_extractor,B3_feature_extractor=weight_add3(B1_feature_extractor,B2_feature_extractor,\n",
    "                                                                                                       G_feature_extractor,B3_feature_extractor,w1,w2,w3)\n",
    "        \n",
    "        weights[\"w1\"].append(np.array(w1))\n",
    "        weights[\"w2\"].append(np.array(w2))\n",
    "        weights[\"w3\"].append(np.array(w3))\n",
    "        weights[\"test2\"].append(np.array(test_loss[\"cr_acc2\"][-1]))\n",
    "        weights[\"test3\"].append(np.array(test_loss[\"cr_acc3\"][-1]))\n",
    "        weights[\"test4\"].append(np.array(test_loss[\"cr_acc4\"][-1]))\n",
    "        print(info1[-1].numpy(),info2[-1].numpy(),info3[-1].numpy(),w1.numpy(),w2.numpy(),w3.numpy(),test_loss[\"cr_acc2\"][-1],test_loss[\"cr_acc3\"][-1],test_loss[\"cr_acc4\"][-1])\n",
    "    return loss1,loss2,loss3,test_loss,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "1.06051 0.4393561 0.54069185 0.8935503 0.04002214 0.06642754 0.871 0.868 0.94025\n",
      "****************************************************************************************************\n",
      "1.130262 1.0143768 1.1692249 0.36032078 0.20185879 0.43782043 0.906 0.92025 0.9705\n",
      "****************************************************************************************************\n",
      "0.95863885 1.3413398 1.2087466 0.08873886 0.60136527 0.30989584 0.907 0.9055 0.9485\n",
      "****************************************************************************************************\n",
      "0.9941937 1.7689666 0.66852146 0.020274002 0.9757472 0.0039787935 0.9505 0.95075 0.99175\n",
      "****************************************************************************************************\n",
      "1.1712208 1.8101406 1.041736 0.038574617 0.9412357 0.020189662 0.93325 0.92225 0.98875\n",
      "****************************************************************************************************\n",
      "1.4336095 1.7639662 0.77714247 0.1599024 0.83409476 0.0060028164 0.9695 0.9655 0.99375\n",
      "****************************************************************************************************\n",
      "1.585753 1.7702277 1.4624882 0.24659827 0.620257 0.13314472 0.94075 0.9415 0.9905\n",
      "****************************************************************************************************\n",
      "1.5052081 1.7486765 1.3976201 0.20152572 0.6807934 0.11768093 0.95625 0.96475 0.99425\n",
      "****************************************************************************************************\n",
      "1.5568838 1.7464384 1.136815 0.27009714 0.69683915 0.03306375 0.958 0.974 0.99625\n",
      "****************************************************************************************************\n",
      "1.5428377 1.7930257 1.3769859 0.2028398 0.7086465 0.08851372 0.9445 0.94125 0.98675\n"
     ]
    }
   ],
   "source": [
    "loss1,loss2,loss3,test_loss,weights=comunication(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}