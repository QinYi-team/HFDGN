{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97581875",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import  Counter\n",
    "import math\n",
    "import logging\n",
    "logging.disable(30)\n",
    "from test_step   import test\n",
    "from train_step4 import train4\n",
    "from MINE import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e79703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one client\n",
    "def weight_add1(model1,test_model):\n",
    "    for i in range(len(model1.weights)):\n",
    "        tf.compat.v1.assign(test_model.weights[i],model1.weights[i])\n",
    "    return model1,test_model\n",
    "# two clients\n",
    "def weight_add2(model1,model2,test_model,w1,w2):\n",
    "    for i in range(len(model1.weights)):\n",
    "        new_weights=tf.reduce_sum([model1.weights[i]*w1,model2.weights[i]*w2],axis=0)\n",
    "        tf.compat.v1.assign(model1.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model2.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(test_model.weights[i],new_weights)\n",
    "    return model1,model2,test_model\n",
    "# three clients\n",
    "def weight_add3(model1,model2,model3,test_model,w1,w2,w3):\n",
    "    for i in range(len(model1.weights)):\n",
    "        new_weights=tf.reduce_sum([model1.weights[i]*w1,model2.weights[i]*w2,model3.weights[i]*w3],axis=0)\n",
    "        tf.compat.v1.assign(model1.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model2.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model3.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(test_model.weights[i],new_weights)\n",
    "    return model1,model2,model3,test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b527d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_name(var):\n",
    "    import inspect\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    var_name_list = [var_name for var_name, var_val in callers_local_vars if var_val is var] \n",
    "    if len(var_name_list) > 0:\n",
    "        return var_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07962d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_preprocess (data):\n",
    "    data=(data-data.min(axis=1).reshape((len(data),1)))/(data.max(axis=1).reshape((len(data),1))-data.min(axis=1).reshape((len(data),1)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8b7850",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 5), (500, 5), (500, 5), (500, 5)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n",
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 4), (500, 4), (500, 4), (500, 4)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n",
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 4), (500, 4), (500, 4), (500, 4)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n",
      "0.5341327554135927 0.5920691489895694 0.5684197615146983 0.7179127363638874\n",
      "<BatchDataset shapes: ((500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 3072, 1), (500, 4), (500, 4), (500, 4), (500, 4)), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "#Gear_dataset\n",
    "G_data1=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load0.csv\").values\n",
    "G_label1=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load0.csv\").values\n",
    "G_data2=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load1.csv\").values\n",
    "G_label2=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load1.csv\").values\n",
    "G_data3=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load2.csv\").values\n",
    "G_label3=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load2.csv\").values\n",
    "G_data4=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\data_load3.csv\").values\n",
    "G_label4=pd.read_csv(r\"F:\\数据集样本划分\\齿轮数据\\DDS试验台--测点 工况\\变负载齿轮数据集-3072\\label_load3.csv\").values\n",
    "G_data1=normalized_preprocess(G_data1) \n",
    "G_data2=normalized_preprocess(G_data2)\n",
    "G_data3=normalized_preprocess(G_data3)\n",
    "G_data4=normalized_preprocess(G_data4)\n",
    "G_data1=tf.expand_dims(G_data1,axis=-1)\n",
    "G_data2=tf.expand_dims(G_data2,axis=-1)\n",
    "G_data3=tf.expand_dims(G_data3,axis=-1)\n",
    "G_data4=tf.expand_dims(G_data4,axis=-1)\n",
    "G_train_dataset=tf.data.Dataset.from_tensor_slices((G_data2,G_data1,G_data3,G_data4,G_label2,G_label1,G_label3,G_label4))\n",
    "G_train_dataset=G_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(G_train_dataset)\n",
    "\n",
    "\n",
    "#Bearing_dataset\n",
    "B_data1=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load0_0.007_test.csv').values\n",
    "B_label1=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load0_0.007_test.csv').values\n",
    "B_data2=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load1_0.007_test.csv').values\n",
    "B_label2=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load1_0.007_test.csv').values\n",
    "B_data3=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load2_0.007_test.csv').values\n",
    "B_label3=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load2_0.007_test.csv').values\n",
    "B_data4=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_data_load3_0.007_test.csv').values\n",
    "B_label4=pd.read_csv(r'F:\\数据集样本划分\\轴承数据\\cwru\\12k_de_cwru_label_load3_0.007_test.csv').values\n",
    "B_data1=normalized_preprocess(B_data1) \n",
    "B_data2=normalized_preprocess(B_data2)\n",
    "B_data3=normalized_preprocess(B_data3)\n",
    "B_data4=normalized_preprocess(B_data4)\n",
    "B_data1=tf.expand_dims(B_data1,axis=-1)\n",
    "B_data2=tf.expand_dims(B_data2,axis=-1)\n",
    "B_data3=tf.expand_dims(B_data3,axis=-1)\n",
    "B_data4=tf.expand_dims(B_data4,axis=-1)\n",
    "B_train_dataset=tf.data.Dataset.from_tensor_slices((B_data1,B_data2,B_data3,B_data4,B_label1,B_label2,B_label3,B_label4))\n",
    "B1_train_dataset=B_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(B1_train_dataset)\n",
    "\n",
    "\n",
    "#Bearing_dataset\n",
    "B_data1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load0_2000_channel1_test.csv\").values\n",
    "B_label1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load0_2000_channel1_test.csv\").values\n",
    "B_data2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load1_2000_channel1_test.csv\").values\n",
    "B_label2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load1_2000_channel1_test.csv\").values\n",
    "B_data3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load2_2000_channel1_test.csv\").values\n",
    "B_label3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load2_2000_channel1_test.csv\").values\n",
    "B_data4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\data_load3_1000_channel1_test.csv\").values\n",
    "B_label4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\cqu\\label_load3_1000_channel1_test.csv\").values\n",
    "B_data1=normalized_preprocess(B_data1) \n",
    "B_data2=normalized_preprocess(B_data2)\n",
    "B_data3=normalized_preprocess(B_data3)\n",
    "B_data4=normalized_preprocess(B_data4)\n",
    "B_data1=tf.expand_dims(B_data1,axis=-1)\n",
    "B_data2=tf.expand_dims(B_data2,axis=-1)\n",
    "B_data3=tf.expand_dims(B_data3,axis=-1)\n",
    "B_data4=tf.expand_dims(B_data4,axis=-1)\n",
    "B_train_dataset=tf.data.Dataset.from_tensor_slices((B_data1,B_data2,B_data3,B_data4,B_label1,B_label2,B_label3,B_label4))\n",
    "B2_train_dataset=B_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(B2_train_dataset)\n",
    "\n",
    "\n",
    "#Bearing_dataset\n",
    "B_data1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load0_1000_0.3_test.csv\").values\n",
    "B_label1=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load0_1000_0.3_test.csv\").values\n",
    "B_data2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load1_1000_0.3_test.csv\").values\n",
    "B_label2=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load1_1000_0.3_test.csv\").values\n",
    "B_data3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load2_1000_0.3_test.csv\").values\n",
    "B_label3=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load2_1000_0.3_test.csv\").values\n",
    "B_data4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\data_load3_1000_0.3_test.csv\").values\n",
    "B_label4=pd.read_csv(r\"F:\\数据集样本划分\\轴承数据\\swju\\label_load3_1000_0.3_test.csv\").values\n",
    "B_data1=normalized_preprocess(B_data1) \n",
    "B_data2=normalized_preprocess(B_data2)\n",
    "B_data3=normalized_preprocess(B_data3)\n",
    "B_data4=normalized_preprocess(B_data4)\n",
    "print(B_data1[0][0],B_data2[0][0],B_data3[0][0],B_data4[0][0])\n",
    "B_data1=tf.expand_dims(B_data1,axis=-1)\n",
    "B_data2=tf.expand_dims(B_data2,axis=-1)\n",
    "B_data3=tf.expand_dims(B_data3,axis=-1)\n",
    "B_data4=tf.expand_dims(B_data4,axis=-1)\n",
    "B_train_dataset=tf.data.Dataset.from_tensor_slices((B_data3,B_data1,B_data2,B_data4,B_label3,B_label1,B_label2,B_label4))\n",
    "B3_train_dataset=B_train_dataset.shuffle(5000).batch(500,drop_remainder=True)\n",
    "print(B3_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a965ff8a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def Feature_extractor():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,kernel_size=16,strides=8,activation='relu',padding='same',input_shape=(3072,1)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv1D(filters=64,kernel_size=3,strides=1,activation='relu',padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same'),])\n",
    "    return model\n",
    "\n",
    "def Disentangler():\n",
    "    input_data=tf.keras.Input(shape=(48,128))\n",
    "    x1=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(input_data)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x1)\n",
    "    x1=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x1)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x1)    \n",
    "    x1=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x1)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    ci_output=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x1)\n",
    "    \n",
    "    x2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(input_data)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x2)\n",
    "    x2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x2)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x2)    \n",
    "    x2=tf.keras.layers.Conv1D(filters=128,kernel_size=3,strides=1,activation='relu',padding='same')(x2)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    cr_output=tf.keras.layers.MaxPooling1D(pool_size=5,strides=2,padding='same')(x2)\n",
    "    \n",
    "    model=tf.keras.Model(inputs=input_data, outputs=[ci_output,cr_output])\n",
    "    return model\n",
    "\n",
    "def Reconstructor():\n",
    "    ci_input=tf.keras.Input(shape=(6,128))\n",
    "    x1=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(ci_input)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x1)\n",
    "    x1=tf.keras.layers.BatchNormalization()(x1)\n",
    "    x1=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x1)\n",
    "    ci_recon=tf.keras.layers.BatchNormalization()(x1)\n",
    "    \n",
    "    cr_input=tf.keras.Input(shape=(6,128))\n",
    "    x2=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(cr_input)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x2)\n",
    "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
    "    x2=tf.keras.layers.Conv1DTranspose(filters=128,kernel_size=3,strides=2,activation='relu',padding='same')(x2)\n",
    "    cr_recon=tf.keras.layers.BatchNormalization()(x2)\n",
    "    \n",
    "    recon_data=tf.keras.layers.Add()([ci_recon,cr_recon])\n",
    "    model=tf.keras.Model(inputs=[ci_input,cr_input], outputs=recon_data)\n",
    "    return model\n",
    "\n",
    "\n",
    "def Classifier(unit_number):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128,activation='relu',input_shape=(128,)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(32,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(unit_number,activation='relu'),])\n",
    "    return model\n",
    "\n",
    "\n",
    "def NET():\n",
    "    model=tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(8,input_shape=(256,),activation='relu'),\n",
    "          tf.keras.layers.Dense(4,activation='relu'),\n",
    "          tf.keras.layers.Dense(1),])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd155ae",
   "metadata": {},
   "source": [
    "def weight_add(model1,model2,test_model,w1,w2):\n",
    "\n",
    "    for i in range(len(model1.weights)):\n",
    "        new_weights=tf.reduce_sum([model1.weights[i]*w1,model2.weights[i]]*w2,axis=0)\n",
    "        tf.compat.v1.assign(model1.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(model2.weights[i],new_weights)\n",
    "        tf.compat.v1.assign(test_model.weights[i],new_weights)\n",
    "    return model1,model2,test_model\n",
    "\n",
    "def comunication(comunication_number):\n",
    "    GAP=tf.keras.layers.GlobalAveragePooling1D()\n",
    "    G_feature_extractor=Feature_extractor()\n",
    "    G_disentangler=Disentangler()\n",
    "    G_reconstructor=Reconstructor()\n",
    "    G_classifier=Classifier(5)\n",
    "    G_net=NET()\n",
    "\n",
    "    B_feature_extractor=Feature_extractor()\n",
    "    B_disentangler=Disentangler()\n",
    "    B_reconstructor=Reconstructor()\n",
    "    B_classifier=Classifier(4)\n",
    "    B_net=NET()\n",
    "\n",
    "    S_feature_extractor=Feature_extractor()\n",
    "    S_disentangler=Disentangler()\n",
    "    S_reconstructor=Reconstructor()\n",
    "    S_classifier=Classifier(4)\n",
    "    S_net=NET()  \n",
    "    \n",
    "    weights={\"w1\":[],\"w2\":[]}\n",
    "    for i in range(comunication_number):\n",
    "        print(\"*\"*100)\n",
    "        loss3,orig_feature,label_feature=test(G_feature_extractor,G_disentangler,G_reconstructor,G_classifier,G_net,5,3,1,1,G_train_dataset,100)\n",
    "        loss1=train4(S_feature_extractor,S_disentangler,S_reconstructor,S_classifier,S_net,5,3,1,1,S_train_dataset,100)\n",
    "        loss2=train4(B_feature_extractor,B_disentangler,B_reconstructor,B_classifier,G_net,5,3,1,1,B_train_dataset,100)\n",
    "        \n",
    "        ci1,cr1= S_disentangler(orig_feature,training = True)\n",
    "        label_feature1=S_classifier(GAP(cr1),training = True)\n",
    "        info1 = Estimator(label_feature1.shape[-1],label_feature.shape[-1]).backward(label_feature1,label_feature,2000)\n",
    "        \n",
    "        ci1,cr1= B_disentangler(orig_feature,training = True)\n",
    "        label_feature2=B_classifier(GAP(cr1),training = True)\n",
    "        info2 = Estimator(label_feature2.shape[-1],label_feature.shape[-1]).backward(label_feature2,label_feature,2000)\n",
    "        \n",
    "        \n",
    "        w1=info1[-1]/(info1[-1]+info2[-1])\n",
    "        w2=info2[-1]/(info1[-1]+info2[-1])\n",
    "        \n",
    "        S_feature_extractor,B_feature_extractor,G_feature_extractor=weight_add(S_feature_extractor,B_feature_extractor,G_feature_extractor,w1,w2)\n",
    "        S_disentangler,B_disentangler,G_disentangler=weight_add(S_disentangler,B_disentangler,G_disentangler,w1,w2)\n",
    "        S_reconstructor,B_reconstructor,G_reconstructor=weight_add(S_reconstructor,B_reconstructor,G_reconstructor,w1,w2)\n",
    "        \n",
    "        weights[\"w1\"].append(np.array(w1))\n",
    "        weights[\"w2\"].append(np.array(w2))\n",
    "        \n",
    "        print(info1[-1].numpy(),info2[-1].numpy(),w1.numpy(),w2.numpy(),loss3[\"cr_acc2\"][-1],loss3[\"cr_acc3\"][-1],loss3[\"cr_acc4\"][-1])\n",
    "    return loss1,loss2,loss3,weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de01af",
   "metadata": {},
   "source": [
    "#two clients\n",
    "def comunication(comunication_number):\n",
    "    GAP=tf.keras.layers.GlobalAveragePooling1D()\n",
    "    G_feature_extractor=Feature_extractor()\n",
    "    G_disentangler=Disentangler()\n",
    "    G_reconstructor=Reconstructor()\n",
    "    G_classifier=Classifier(5)\n",
    "    G_net=NET()\n",
    "\n",
    "    B_feature_extractor=Feature_extractor()\n",
    "    B_disentangler=Disentangler()\n",
    "    B_reconstructor=Reconstructor()\n",
    "    B=Classifier(4)\n",
    "    B_net=NET()\n",
    "\n",
    "    S_feature_extractor=Feature_extractor()\n",
    "    S=Disentangler()\n",
    "    S_reconstructor=Reconstructor()\n",
    "    S_classifier=Classifier(4)\n",
    "    S_net=NET() \n",
    "    \n",
    "    weights={\"w1\":[],\"w2\":[]}\n",
    "    for i in range(comunication_number):\n",
    "        print(\"*\"*100)\n",
    "        loss3,orig_feature,label_feature=test(G_feature_extractor,G_disentangler,G_reconstructor,G_classifier,G_net,5,3,1,1,G_train_dataset,100)\n",
    "        loss1=train2(S_feature_extractor,S_disentangler,S_reconstructor,S_classifier,S_net,5,3,1,1,S_train_dataset,100)\n",
    "        loss2=train2(B_feature_extractor,B_disentangler,B_reconstructor,B_classifier,B_net,5,3,1,1,B_train_dataset,100)\n",
    "        \n",
    "        ci1,cr1= S_disentangler(orig_feature,training = True)\n",
    "        label_feature1=S_classifier(GAP(cr1),training = True)\n",
    "        info1 = Estimator(label_feature1.shape[-1],label_feature.shape[-1]).backward(label_feature1,label_feature,2000)\n",
    "        \n",
    "        ci1,cr1= B_disentangler(orig_feature,training = True)\n",
    "        label_feature2=B_classifier(GAP(cr1),training = True)\n",
    "        info2 = Estimator(label_feature2.shape[-1],label_feature.shape[-1]).backward(label_feature2,label_feature,2000)\n",
    "        \n",
    "        w1=tf.nn.softmax([info1[-1]/0.1,info2[-1]/0.1],axis=0)[0]\n",
    "        w2=tf.nn.softmax([info1[-1]/0.1,info2[-1]/0.1],axis=0)[1]\n",
    "        \n",
    "        #w1=info1[-1]/(info1[-1]+info2[-1])\n",
    "        #w2=info2[-1]/(info1[-1]+info2[-1])\n",
    "        #print(S_feature_extractor.weights[0][0],S_feature_extractor.weights[0][0]*w1,B_feature_extractor.weights[0][0],B_feature_extractor.weights[0][0]*w2)\n",
    "        \n",
    "        S_feature_extractor,B_feature_extractor,G_feature_extractor=weight_add(S_feature_extractor,B_feature_extractor,G_feature_extractor,w1,w2)\n",
    "        #S_disentangler,B_disentangler,G_disentangler=weight_add(S_disentangler,B_disentangler,G_disentangler,w1,w2)\n",
    "        #S_reconstructor,B_reconstructor,G_reconstructor=weight_add(S_reconstructor,B_reconstructor,G_reconstructor,w1,w2)\n",
    "        \n",
    "        #print(S_feature_extractor.weights[0][0],B_feature_extractor.weights[0][0],G_feature_extractor.weights[0][0])\n",
    "        \n",
    "        weights[\"w1\"].append(np.array(w1))\n",
    "        weights[\"w2\"].append(np.array(w2))\n",
    "        \n",
    "        print(info1[-1].numpy(),info2[-1].numpy(),w1.numpy(),w2.numpy(),loss3[\"cr_acc2\"][-1],loss3[\"cr_acc3\"][-1],loss3[\"cr_acc4\"][-1])\n",
    "    return loss1,loss2,loss3,weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb52df",
   "metadata": {},
   "source": [
    "path=r\"C:\\Users\\帅哥泉\\Desktop\\HFDGN\\data_collection\\B4B4B4_G4\\1\"\n",
    "if os.path.exists(path)==False:\n",
    "    os.makedirs(path)\n",
    "for i in [loss1,loss2,loss3,weights]:\n",
    "    df=pd.DataFrame(pd.DataFrame.from_dict(i, orient='index').values.T, columns=list(i.keys()))\n",
    "    df.to_csv(path+r\"\\{}.csv\".format(retrieve_name(i)),index=True)\n",
    "    for j in i.keys():\n",
    "        plt.plot(i[j],label=j)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ebcfb3",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def comunication(comunication_number):\n",
    "    GAP=tf.keras.layers.GlobalAveragePooling1D()\n",
    "    G_feature_extractor=Feature_extractor()\n",
    "    G_disentangler=Disentangler()\n",
    "    G_reconstructor=Reconstructor()\n",
    "    G_classifier=Classifier(5)\n",
    "    G_net=NET()\n",
    "\n",
    "    B1_feature_extractor=Feature_extractor()\n",
    "    B1_disentangler=Disentangler()\n",
    "    B1_reconstructor=Reconstructor()\n",
    "    B1_classifier=Classifier(4)\n",
    "    B1_net=NET()\n",
    "\n",
    "    B2_feature_extractor=Feature_extractor()\n",
    "    B2_disentangler=Disentangler()\n",
    "    B2_reconstructor=Reconstructor()\n",
    "    B2_classifier=Classifier(4)\n",
    "    B2_net=NET() \n",
    "    \n",
    "    B3_feature_extractor=Feature_extractor()\n",
    "    B3_disentangler=Disentangler()\n",
    "    B3_reconstructor=Reconstructor()\n",
    "    B3_classifier=Classifier(4)\n",
    "    B3_net=NET()\n",
    "    \n",
    "    weights={\"w1\":[],\"w2\":[],\"w3\":[],\"test2\":[],\"test3\":[],\"test4\":[]}\n",
    "    for i in range(comunication_number):\n",
    "        print(\"*\"*100)\n",
    "        loss1=train4(B1_feature_extractor,B1_disentangler,B1_reconstructor,B1_classifier,B1_net,5,1,1,1,B1_train_dataset,100)\n",
    "        loss2=train4(B2_feature_extractor,B2_disentangler,B2_reconstructor,B2_classifier,B2_net,5,1,1,1,B2_train_dataset,100)\n",
    "        loss3=train4(G_feature_extractor,G_disentangler,G_reconstructor,G_classifier,G_net,5,1,1,1,G_train_dataset,100)\n",
    "        test_loss,orig_feature,label_feature=test(B3_feature_extractor,B3_disentangler,B3_reconstructor,B3_classifier,B3_net,5,1,1,1,B3_train_dataset,100)        \n",
    "        \n",
    "        ci1,cr1= B1_disentangler(orig_feature,training = True)\n",
    "        label_feature1=B1_classifier(GAP(cr1),training = True)\n",
    "        info1 = Estimator(label_feature1.shape[-1],label_feature.shape[-1]).backward(label_feature1,label_feature,2000)\n",
    "        \n",
    "        ci1,cr1= B2_disentangler(orig_feature,training = True)\n",
    "        label_feature2=B2_classifier(GAP(cr1),training = True)\n",
    "        info2 = Estimator(label_feature2.shape[-1],label_feature.shape[-1]).backward(label_feature2,label_feature,2000)\n",
    " \n",
    "        ci1,cr1= G_disentangler(orig_feature,training = True)\n",
    "        label_feature3=G_classifier(GAP(cr1),training = True)\n",
    "        info3 = Estimator(label_feature3.shape[-1],label_feature.shape[-1]).backward(label_feature3,label_feature,2000)\n",
    "        \n",
    "        c=0.2\n",
    "        w1=tf.nn.softmax([info1[-1]/c,info2[-1]/c,info3[-1]/c],axis=0)[0]\n",
    "        w2=tf.nn.softmax([info1[-1]/c,info2[-1]/c,info3[-1]/c],axis=0)[1]\n",
    "        w3=tf.nn.softmax([info1[-1]/c,info2[-1]/c,info3[-1]/c],axis=0)[2]\n",
    "        \n",
    "        B1_feature_extractor,B2_feature_extractor,G_feature_extractor,B3_feature_extractor=weight_add3(B1_feature_extractor,B2_feature_extractor,G_feature_extractor,B3_feature_extractor,w1,w2,w3)\n",
    "        #S_disentangler,B_disentangler,G_disentangler=weight_add(S_disentangler,B_disentangler,G_disentangler,w1,w2)\n",
    "        #S_reconstructor,B_reconstructor,G_reconstructor=weight_add(S_reconstructor,B_reconstructor,G_reconstructor,w1,w2)\n",
    "        #print(S_feature_extractor.weights[0][0],B_feature_extractor.weights[0][0],G_feature_extractor.weights[0][0])\n",
    "        \n",
    "        weights[\"w1\"].append(np.array(w1))\n",
    "        weights[\"w2\"].append(np.array(w2))\n",
    "        weights[\"w3\"].append(np.array(w3))\n",
    "        weights[\"test2\"].append(np.array(test_loss[\"cr_acc2\"][-1]))\n",
    "        weights[\"test3\"].append(np.array(test_loss[\"cr_acc3\"][-1]))\n",
    "        weights[\"test4\"].append(np.array(test_loss[\"cr_acc4\"][-1]))\n",
    "        print(info1[-1].numpy(),info2[-1].numpy(),info3[-1].numpy(),w1.numpy(),w2.numpy(),w3.numpy(),test_loss[\"cr_acc2\"][-1],test_loss[\"cr_acc3\"][-1],test_loss[\"cr_acc4\"][-1])\n",
    "    return loss1,loss2,loss3,test_loss,weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24aebfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a7f524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "test accuracy: 0.024690091609954834\n",
      "test accuracy: 0.026436150074005127\n",
      "test accuracy: 0.045129358768463135\n",
      "test accuracy: 0.0261804461479187\n",
      "test accuracy: 0.025688111782073975\n",
      "test accuracy: 0.024963021278381348\n",
      "test accuracy: 0.02525538206100464\n",
      "test accuracy: 0.02519357204437256\n",
      "test accuracy: 0.02628880739212036\n",
      "test accuracy: 0.02443397045135498\n",
      "test accuracy: 0.024471282958984375\n",
      "test accuracy: 0.024926364421844482\n",
      "test accuracy: 0.024940848350524902\n",
      "test accuracy: 0.024933576583862305\n",
      "test accuracy: 0.02517455816268921\n",
      "test accuracy: 0.02539503574371338\n",
      "test accuracy: 0.025434792041778564\n",
      "test accuracy: 0.02592933177947998\n",
      "test accuracy: 0.025431931018829346\n",
      "test accuracy: 0.026428937911987305\n",
      "test accuracy: 0.044622838497161865\n",
      "test accuracy: 0.02468407154083252\n",
      "test accuracy: 0.025182604789733887\n",
      "test accuracy: 0.024690866470336914\n",
      "test accuracy: 0.026012659072875977\n",
      "test accuracy: 0.025680959224700928\n",
      "test accuracy: 0.02767622470855713\n",
      "test accuracy: 0.02572154998779297\n",
      "test accuracy: 0.025758087635040283\n",
      "test accuracy: 0.02568155527114868\n",
      "test accuracy: 0.026436567306518555\n",
      "test accuracy: 0.025185883045196533\n",
      "test accuracy: 0.026685476303100586\n",
      "test accuracy: 0.026751458644866943\n",
      "test accuracy: 0.025409340858459473\n",
      "test accuracy: 0.026430189609527588\n",
      "test accuracy: 0.025674521923065186\n",
      "test accuracy: 0.0261000394821167\n",
      "test accuracy: 0.02658247947692871\n",
      "test accuracy: 0.025930345058441162\n",
      "0.92516416 0.8212426 0.5193613 0.5793008 0.3445413 0.07615783 0.90625 0.908 0.96325\n"
     ]
    }
   ],
   "source": [
    "loss1,loss2,loss3,test_loss,weights=comunication(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5947eb",
   "metadata": {},
   "source": [
    "for j in range(1):\n",
    "    print(\"#\"*50)\n",
    "    loss1,loss2,loss3,test_loss,weights=comunication(10) \n",
    "    path=r\"C:\\Users\\帅哥泉\\Desktop\\HFDGN\\data_collection\\CWRU+CQU+DDS-SWJTU\\B4B4G4_B4\\B3-B1B2B4\\{}\".format(j+1)\n",
    "    if os.path.exists(path)==False:\n",
    "        os.makedirs(path)\n",
    "    for i in [loss1,loss2,loss3,test_loss,weights]:\n",
    "        df=pd.DataFrame(pd.DataFrame.from_dict(i, orient='index').values.T, columns=list(i.keys()))\n",
    "        df.to_csv(path+r\"\\{}.csv\".format(retrieve_name(i)),index=True) \n",
    "        #for j in i.keys():\n",
    "            #plt.plot(i[j],label=j)\n",
    "            #plt.legend()\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d3859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fecd43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 336.6545705795288\n"
     ]
    }
   ],
   "source": [
    "print(\"training time:\",end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
